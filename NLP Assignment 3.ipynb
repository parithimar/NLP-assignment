{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f50d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A Recurrent Neural Network (RNN) is a class of Artificial Neural Network in which the connection between different nodes forms a directed graph to give a temporal dynamic behavior. It helps to model sequential data that are derived from feedforward networks. It works similarly to human brains to deliver predictive results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c267eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Backpropagation Through Time, or BPTT, is the training algorithm used to update weights in recurrent neural networks like LSTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d682e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "In contrast to the vanishing gradients problem, exploding gradients occur as a result of the weights in the network and not the activation function. The weights in the lower layers are more likely to be affected by exploding gradients as their associated gradients are products of more values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9891787",
   "metadata": {},
   "outputs": [],
   "source": [
    "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems. This is a behavior required in complex problem domains like machine translation, speech recognition, and more. LSTMs are a complex area of deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a03cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gated recurrent unit s ( GRU s) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956abf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are many other kinds of LSTMs as well. The figure on the right is a graphical representation of an LSTM unit with peephole connections (i.e. a peephole LSTM). Peephole connections allow the gates to access the constant error carousel (CEC), whose activation is the cell state. h t âˆ’ 1 {displaystyle h_ {t-1}}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51611c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Image: kdnuggets.com\n",
    "Bidirectional RNNs To enable straight (past) and reverse traversal of input (future), Bidirectional RNNs, or BRNNs, are used. A BRNN is a combination of two RNNs - one RNN moves forward, beginning from the start of the data sequence, and the other, moves backward, beginning from the end of the data sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e85cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "So we have three different gates that regulate information flow in an LSTM cell. A forget gate, input gate, and output gate. First, we have the forget gate. This gate decides what information should be thrown away or kept. Information from the previous hidden state and information from the current input is passed through the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79cbbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "A Bidirectional LSTM, or biLSTM, is a sequence processing model that consists of two LSTMs: one taking the input in a forward direction, and the other in a backwards direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb6dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "A Bidirectional GRU, or BiGRU, is a sequence processing model that consists of two GRUs. one taking the input in a forward direction, and the other in a backwards direction. It is a bidirectional recurrent neural network with only the input and forget gates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
