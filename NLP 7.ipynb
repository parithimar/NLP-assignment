{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae03361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenisation. My dog is cute. He likes playing. You basically understand that tokeninsation consists in removings...\n",
    "Embedding. Embedding is a vectorisation. There are many algorithm for Embedding, and BERT use WordPiece embeddings.\n",
    "Transformer block. This part is quite long and difficult. ... If you don’t have time for these papers, here what you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6792573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Masked Language Modeling (MLM) is a language task very common in Transformer architectures today. It involves masking part of the input, then learning a model to predict the missing tokens – essentially reconstructing the non-masked input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16880507",
   "metadata": {},
   "outputs": [],
   "source": [
    "Next sentence prediction ( NSP) is another interesting strategy used for training the BERT model. NSP is a binary classification task. In the NSP task, we feed two sentences to BERT and it has to predict whether the second sentence is the follow-up (next sentence) of the first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2bf69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Matthews Correlation Coefficient The Matthews Correlation Coefficient (MCC) has a range of -1 to 1 where -1 indicates a completely wrong binary classifier while 1 indicates a completely correct binary classifier. Using the MCC allows one to gauge how well their classification model/function is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e97064",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Matthews Correlation Coefficient (MCC) has a range of -1 to 1 where -1 indicates a completely wrong binary classifier while 1 indicates a completely correct binary classifier. Using the MCC allows one to gauge how well their classification model/function is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32dcab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "In natural language processing, semantic role labeling (also called shallow semantic parsing or slot-filling) is the process that assigns labels to words or phrases in a sentence that indicates their semantic role in the sentence, such as that of an agent, goal, or result. It serves to find the meaning of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7410acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "By fine-tuning BERT, we are now able to get away with training a model to good performance on a much smaller amount of training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be9040",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntroduction. Given two text fragments called ‘Text’ and ‘Hypothesis’, Textual Entailment Recognition is the task of...\n",
    "RTE-6 Tasks. The RTE-6 tasks focus on recognizing textual entailment in two application settings: Summarization and...\n",
    "Schedule.\n",
    "Mailing List. The mailing list for the RTE Track is rte@nist.gov. The list is used to discuss and define the task..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67835e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "The GPT is a 12-layer decoder only transformer with 117M parameters. The Transformer has a stack of 6 Encoder and 6 Decoder, unlike Seq2Seq; the Encoder contains two sub-layers: multi-head self-attention layer and a fully connected feed-forward network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
