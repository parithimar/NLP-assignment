{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9456b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sequence-to-Sequence Basics. A basic sequence-to-sequence model, as introduced in Cho et al., 2014 ( pdf ), consists of two recurrent neural networks (RNNs): an encoder that processes the input and ...\n",
    "TensorFlow seq2seq Library. As you can see above, there are many different sequence-to-sequence models. ...\n",
    "Neural Translation Model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65822cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s you remember, the gradient descent algorithm finds the global minimum of the cost function that is going to be an optimal setup for the network.\n",
    "\n",
    "As you might also recall, information travels through the neural network from input neurons to the output neurons, while the error is calculated and propagated back through the network to update the weights.\n",
    "\n",
    "It works quite similarly for RNNs, but here we’ve got a little bit more going on.\n",
    "\n",
    "Firstly, information travels through time in RNNs, which means that information from previous time points is used as input for the next time points.\n",
    "Secondly, you can calculate the cost function, or your error, at each time point.\n",
    "\n",
    "Basically, during the training, your cost function compares your outcomes (red circles on the image below) to your desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774cf1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient clipping is a technique used in deep learning to optimize and solve problems. Deep learning is a subfield of machine learning that uses algorithms inspired by the structure and function of the human brain and neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f6dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Attention mechanism in Deep Learning is based off this concept of directing your focus, and it pays greater attention to certain factors when processing the data. In broad terms, Attention is one component of a network’s architecture, and is in charge of managing and quantifying the interdependence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394fc9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Introduction: Let’s say we want to build an application where we predict an output vector y = {y0, y1, ………, yn} of a...\n",
    "Generative versus Discriminative Models: Generative models are models that describe how a label vector y can...\n",
    "CRF for sequence models: The power of CRF models comes to rescue when the model predicts many variables that are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2e4c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Take each word of input sentence and generate the embedding from it.\n",
    "In this mechanism, we created h (h = 8) different attention heads, each head has different weight matrices (W (Q), W...\n",
    "In this step, we multiply the input matrix with each of the weight matrices (W Q, W K, W V) to produce the key, value,...\n",
    "Now, we apply the attention mechanism to these query, key, and value matrices, this gives us an output matrix from each.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac6b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bahdanau Attention is also known as Additive attention as it performs a linear combination of encoder states and the decoder states. Now, let’s understand the mechanism suggested by Bahdanau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9ff674",
   "metadata": {},
   "outputs": [],
   "source": [
    "-gram. N-grams are a relatively simple approach to language models. ...\n",
    "Unigram. The unigram is the simplest type of language model. ...\n",
    "Bidirectional. Unlike n-gram models, which analyze text in one direction (backwards), bidirectional models analyze text in both directions, backwards and forwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46add6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multi-head Attention is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739763bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
