{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda55868",
   "metadata": {},
   "outputs": [],
   "source": [
    "One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction. So, you’re playing with ML models and you encounter this “One hot encoding” term all over the place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06cf4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bag of Words (BOW) is a method to extract features from text documents. These features can be used for training machine learning algorithms. It creates a vocabulary of all the unique words occurring in all the documents in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1213ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "N-Grams is an important concept to understand in text analytics. Essentially, N-Grams is a set of 1 or more consecutive sequence of items that occur next to each other. As mentioned above, N is a numerical value that implies the n items of sequence of text. When we type text in a search engine, we can see the probabilistic model of the search engine starts predicting the next set of words based on the contex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e80aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF-IDF from scratch in python on real world dataset. What is TF-IDF? Preprocessing data. Weights to title and body. Document retrieval using TF-IDF matching score. Document retrieval using TF-IDF cosine similarity. TF-IDF stands for “Term Frequency — Inverse Document Frequency”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674895f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "These words are often called out-of-vocabulary (OOV) words. These OOV words may pose a problem: since OOV words never occur in the training data, the model never learns their embeddings, and thus cannot represent them as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab810306",
   "metadata": {},
   "outputs": [],
   "source": [
    "Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ffda41",
   "metadata": {},
   "outputs": [],
   "source": [
    "Continuous Bag of Words (CBOW): It attempts to guess the output (target word) from its neighboring words (context words). You can think of it like fill in the blank task, where you need to guess word in place of blank by observing nearby words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64476fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Skip-gram model is basically the inverse of the CBOW model. The input is a centre word and the model predicts the context words. In this section we will be implementing the Skipgram for multi-word architecture of Word2Vec. Like single word CBOW and multi word CBOW the content is broken down into the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfda013",
   "metadata": {},
   "outputs": [],
   "source": [
    "GloVe (Global Vectors for Word Representation) is an alternate method to create word embeddings. It is based on matrix factorization techniques on the word-context matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
